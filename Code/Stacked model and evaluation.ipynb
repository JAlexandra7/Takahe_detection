{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1b788e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Citations\n",
    "# GBIF.org (15 June 2025) GBIF Occurrence Download  https://doi.org/10.15468/dl.mhxdcy\n",
    "# Data was obtained from GBIF-NZ, it includes records from 6 datasets: iNaturalist Research-grade Observations Observation.org, Nature data from around the World, Auckland Museum Land Vertebrates Collection, Xeno-canto - Bird sounds from around the world, NABU|naturgucker, MVZ Egg and Nest Collection (Arctos)\n",
    "# There were 4258 occurances in the original dataset, but this was reduced to 4202 occurances as a result of data cleaning.\n",
    "# Of the original 4258 occurances, 581 were takahe (Porphyrio hochstetteri) and 3621 were pukeko (Porphyrio melanotus subsp. melanotus).\n",
    "# In the cleaned dataset 550 were takahe (Porphyrio hochstetteri) and 3557 were pukeko (Porphyrio melanotus subsp. melanotus).\n",
    "# The data cleaning and downloading of the images was performed in Rstudio.\n",
    "# Made using Python 3.12.9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f0803183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading packages\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Subset, random_split, Dataset, TensorDataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from torchvision.models import efficientnet_b3, EfficientNet_B3_Weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bd8569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the data by resizing the images, converting the images to tensors and normalizing the images\n",
    "Transform_data = transforms.Compose([\n",
    "    # Resizes images to 600 by 600 pixels\n",
    "    transforms.Resize((600, 600)),\n",
    "    # randomly horizontally flips images\n",
    "    transforms.RandomHorizontalFlip(p = 0.3),\n",
    "    # randomly rotates images\n",
    "    transforms.RandomRotation(degrees = (0, 180)),\n",
    "    # Randomly alters the visual appearance of the images during training\n",
    "    transforms.ColorJitter(brightness = 0.2, contrast = 0.2, saturation = 0.2, hue = 0.1),\n",
    "    # turns images into tensors\n",
    "    transforms.ToTensor(),\n",
    "    # Normalises the data\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Random data augmentation techniques are not used on the validation set to avoid overfitting\n",
    "V_Transform_data = transforms.Compose([\n",
    "    # Resizes images to 600 by 600 pixels\n",
    "    transforms.Resize((600, 600)),\n",
    "    # turns images into tensors\n",
    "    transforms.ToTensor(),\n",
    "    # Normalises the data\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Importing data and splitting the dataset into training, validation and testing sets\n",
    "Images = ImageFolder(root = \"Train_data\", transform = None)\n",
    "\n",
    "# Splitting into train/val/test once, capturing indices\n",
    "total_size = len(Images)\n",
    "train_size = int(0.7 * total_size)\n",
    "val_size = int(0.15 * total_size)\n",
    "test_size = total_size - train_size - val_size  # Ensures total sums correctly\n",
    "\n",
    "# Setting a seed to ensure reproducible results\n",
    "generator = torch.Generator().manual_seed(0)\n",
    "\n",
    "training_subset, validation_subset, Test_subset = random_split(Images, [train_size, val_size, test_size], generator = generator)\n",
    "train_idx = training_subset.indices\n",
    "val_idx   = validation_subset.indices\n",
    "test_idx  = Test_subset.indices\n",
    "\n",
    "# Creating three ImageFolder datasets with transforms, and sub-indexing them:\n",
    "train_ds = Subset(ImageFolder(\"Train_data\", transform = Transform_data), train_idx)\n",
    "val_ds   = Subset(ImageFolder(\"Train_data\", transform = V_Transform_data),  val_idx)\n",
    "test_ds  = Subset(ImageFolder(\"Train_data\", transform = V_Transform_data),  test_idx)\n",
    "\n",
    "# Build the data loaders\n",
    "train_loader = DataLoader(train_ds, batch_size = 35, shuffle = True)\n",
    "validation_loader = DataLoader(val_ds, batch_size = 35, shuffle = False)\n",
    "test_loader  = DataLoader(test_ds, batch_size = 35, shuffle = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07a4f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the CNN model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Feature extraction - convolution layer generates a feature map\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 16, kernel_size = 3, stride = 2, padding = 1)\n",
    "        # Batch normalisation\n",
    "        self.batch1 = nn.BatchNorm2d(16, momentum = 0.1, affine = True)\n",
    "        # Using pooling for first conv, then stride = 2 on 2nd conv, then pooling for thrid conv\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 3, stride = 1)\n",
    "        self.batch2 = nn.BatchNorm2d(32, momentum = 0.1, affine = True)\n",
    "        self.pool = nn.MaxPool2d(kernel_size = 5, stride = 5, padding = 0, ceil_mode = True)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3, stride = 2)\n",
    "        self.batch3 = nn.BatchNorm2d(64, momentum = 0.1, affine = True)\n",
    "        \n",
    "        self.pool2 = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # Applies a linear transformation to the data\n",
    "        self.fc1 = nn.Linear(in_features = 64, out_features = 32)\n",
    "        self.batchfc1 = nn.BatchNorm1d(32)\n",
    "        self.fc2 = nn.Linear(in_features = 32, out_features = 16)\n",
    "        self.batchfc2 = nn.BatchNorm1d(16)\n",
    "        self.fc3 = nn.Linear(in_features = 16, out_features = 8)\n",
    "        self.batchfc3 = nn.BatchNorm1d(8)\n",
    "        # out_features corresponds to the number of classes\n",
    "        self.fc4 = nn.Linear(in_features = 8, out_features = 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # blocks: conv, batch, relu, pooling\n",
    "        x = F.relu(self.pool(self.batch1(self.conv1(x))))\n",
    "        x = F.relu(self.batch2(self.conv2(x)))\n",
    "        x = F.relu(self.pool(self.batch3(self.conv3(x))))\n",
    "        # Flattens input 'x' by reshaping it into a one dimensional tensor.\n",
    "        x = self.pool2(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        # linear\n",
    "        x = F.relu(self.batchfc1(self.fc1(x)))\n",
    "        x = F.relu(self.batchfc2(self.fc2(x)))\n",
    "        x = F.relu(self.batchfc3(self.fc3(x)))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "modelA = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "efbe757e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining model B - only difference between modelA and B are the class weights\n",
    "modelB = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8705aa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining model C\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Loading pretrained EfficientNet-B3\n",
    "weights = EfficientNet_B3_Weights.IMAGENET1K_V1\n",
    "modelC = efficientnet_b3(weights=weights)\n",
    "\n",
    "# Replacing the classifier head for 2 classes: EfficientNet_B3 has `model.classifier = nn.Sequential( Dropout, Linear )`\n",
    "in_features = modelC.classifier[1].in_features\n",
    "modelC.classifier[1] = nn.Linear(in_features, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd93cca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(40, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(10, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(24, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(6, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.007692307692307693, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.015384615384615385, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.02307692307692308, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.03076923076923077, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=192, bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.038461538461538464, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)\n",
       "            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.04615384615384616, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)\n",
       "            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.05384615384615385, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)\n",
       "            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.06153846153846154, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.06923076923076923, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07692307692307693, mode=row)\n",
       "      )\n",
       "      (3): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.08461538461538462, mode=row)\n",
       "      )\n",
       "      (4): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.09230769230769233, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(576, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n",
       "            (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1076923076923077, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n",
       "            (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.11538461538461539, mode=row)\n",
       "      )\n",
       "      (3): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n",
       "            (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.12307692307692308, mode=row)\n",
       "      )\n",
       "      (4): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n",
       "            (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.13076923076923078, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(816, 816, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=816, bias=False)\n",
       "            (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(816, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.13846153846153847, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
       "            (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.14615384615384616, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
       "            (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.15384615384615385, mode=row)\n",
       "      )\n",
       "      (3): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
       "            (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.16153846153846155, mode=row)\n",
       "      )\n",
       "      (4): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
       "            (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.16923076923076924, mode=row)\n",
       "      )\n",
       "      (5): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
       "            (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.17692307692307693, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1392, 1392, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1392, bias=False)\n",
       "            (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1392, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.18461538461538465, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(2304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)\n",
       "            (1): BatchNorm2d(2304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(2304, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(96, 2304, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.19230769230769232, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (8): Conv2dNormActivation(\n",
       "      (0): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.3, inplace=True)\n",
       "    (1): Linear(in_features=1536, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# loading trained models\n",
    "modelA.load_state_dict(torch.load(\"Models/modelA.pth\"))\n",
    "modelB.load_state_dict(torch.load(\"Models/modelB.pth\"))\n",
    "modelC.load_state_dict(torch.load(\"Models/modelC.pth\"))\n",
    "modelA.to(device)\n",
    "modelB.to(device)\n",
    "modelC.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837d415d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using raw probabilities:\n",
    "modelA.eval()\n",
    "modelB.eval()\n",
    "modelC.eval()\n",
    "all_preds  = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in validation_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        # Raw logits\n",
    "        outA = modelA(inputs)\n",
    "        outB = modelB(inputs)\n",
    "        outC = modelC(inputs)\n",
    "\n",
    "        # Raw class-1 probabilities\n",
    "        pA = torch.softmax(outA, dim = 1)[:, 1]\n",
    "        pB = torch.softmax(outB, dim = 1)[:, 1]\n",
    "        pC = torch.softmax(outC, dim = 1)[:, 1]\n",
    "\n",
    "        # Stacking the floats, shape [batch,2]\n",
    "        stacked_input = torch.stack([pA, pB, pC], dim=1) # Shape: [batch, num_models]\n",
    "\n",
    "        all_preds.append(stacked_input.cpu())\n",
    "        all_labels.append(targets.cpu())\n",
    "\n",
    "final_preds  = torch.cat(all_preds)   # [N_val, 3] where 3 is the number of models\n",
    "final_labels = torch.cat(all_labels)  # [N_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "31d8b267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_preds.shape: torch.Size([616, 3])\n",
      "final_labels.shape: torch.Size([616])\n"
     ]
    }
   ],
   "source": [
    "# Checking that the final_pred and final_labels match size\n",
    "print(\"final_preds.shape:\", final_preds.shape)\n",
    "print(\"final_labels.shape:\", final_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "252a39d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03909480571746826, 0.01853853650391102, 0.46096596121788025] → 1 samples\n",
      "[0.06400875747203827, 0.038229119032621384, 0.017123812809586525] → 1 samples\n",
      "[0.09014938026666641, 0.05046525225043297, 0.08127690851688385] → 1 samples\n",
      "[0.09074638783931732, 0.03855164349079132, 0.13635680079460144] → 1 samples\n",
      "[0.09144870191812515, 0.02581791952252388, 0.04538532346487045] → 1 samples\n",
      "[0.10982140153646469, 0.0642043724656105, 0.01951802708208561] → 1 samples\n",
      "[0.1123390719294548, 0.07824064046144485, 0.14137162268161774] → 1 samples\n",
      "[0.11412861198186874, 0.057338327169418335, 0.07877081632614136] → 1 samples\n",
      "[0.11696574091911316, 0.05548581853508949, 0.02731083519756794] → 1 samples\n",
      "[0.1228940486907959, 0.04109540954232216, 0.0666346400976181] → 1 samples\n",
      "[0.12729845941066742, 0.07667181640863419, 0.030236901715397835] → 1 samples\n",
      "[0.1294945776462555, 0.04713558405637741, 0.010667994618415833] → 1 samples\n",
      "[0.1327301561832428, 0.11861240118741989, 0.36970406770706177] → 1 samples\n",
      "[0.1358194351196289, 0.09948992729187012, 0.03602442517876625] → 1 samples\n",
      "[0.13699555397033691, 0.07317809015512466, 0.14365381002426147] → 1 samples\n",
      "[0.13989797234535217, 0.02942931279540062, 0.029088599607348442] → 1 samples\n",
      "[0.14262914657592773, 0.039275337010622025, 0.01804431341588497] → 1 samples\n",
      "[0.14400377869606018, 0.17821766436100006, 0.16910187900066376] → 1 samples\n",
      "[0.14733479917049408, 0.12305528670549393, 0.5247930288314819] → 1 samples\n",
      "[0.14996933937072754, 0.11627040803432465, 0.2790911793708801] → 1 samples\n",
      "[0.15730823576450348, 0.061300281435251236, 0.014803006313741207] → 1 samples\n",
      "[0.16282513737678528, 0.158967986702919, 0.34530743956565857] → 1 samples\n",
      "[0.1638614386320114, 0.13797757029533386, 0.04657034948468208] → 1 samples\n",
      "[0.1662730574607849, 0.1363515406847, 0.0017626691842451692] → 1 samples\n",
      "[0.1663823425769806, 0.04992680624127388, 0.1360674351453781] → 1 samples\n",
      "[0.1703479290008545, 0.15358379483222961, 0.13007290661334991] → 1 samples\n",
      "[0.17258651554584503, 0.1940661072731018, 0.1797441840171814] → 1 samples\n",
      "[0.17430855333805084, 0.10111714899539948, 0.04657401144504547] → 1 samples\n",
      "[0.17717091739177704, 0.06976759433746338, 0.1872110813856125] → 1 samples\n",
      "[0.17720720171928406, 0.2596552073955536, 0.30071350932121277] → 1 samples\n",
      "[0.17767828702926636, 0.10776174068450928, 0.30907273292541504] → 1 samples\n",
      "[0.17983655631542206, 0.07872343808412552, 0.0865769162774086] → 1 samples\n",
      "[0.1825685203075409, 0.18432193994522095, 0.09688255190849304] → 1 samples\n",
      "[0.1862761378288269, 0.21607166528701782, 0.25488588213920593] → 1 samples\n",
      "[0.18747670948505402, 0.2089644968509674, 0.01453943271189928] → 1 samples\n",
      "[0.18848609924316406, 0.16586396098136902, 0.4158075451850891] → 1 samples\n",
      "[0.19137944281101227, 0.17085273563861847, 0.047441475093364716] → 1 samples\n",
      "[0.19540433585643768, 0.10644093155860901, 0.037145499140024185] → 1 samples\n",
      "[0.1959441751241684, 0.052296802401542664, 0.13558828830718994] → 1 samples\n",
      "[0.19962428510189056, 0.10493648052215576, 0.2371506541967392] → 1 samples\n",
      "[0.19975021481513977, 0.1313830018043518, 0.45570236444473267] → 1 samples\n",
      "[0.20079012215137482, 0.11724866181612015, 0.038759514689445496] → 1 samples\n",
      "[0.20134906470775604, 0.2097451388835907, 0.40118712186813354] → 1 samples\n",
      "[0.20253339409828186, 0.20050667226314545, 0.27905628085136414] → 1 samples\n",
      "[0.20288433134555817, 0.20296324789524078, 0.010085396468639374] → 1 samples\n",
      "[0.20400793850421906, 0.0792073979973793, 0.1580745428800583] → 1 samples\n",
      "[0.20610393583774567, 0.16377459466457367, 0.004181916359812021] → 1 samples\n",
      "[0.20905283093452454, 0.12656284868717194, 0.0012795785441994667] → 1 samples\n",
      "[0.21005471050739288, 0.18682275712490082, 0.3471556603908539] → 1 samples\n",
      "[0.21077004075050354, 0.2035500556230545, 0.040883056819438934] → 1 samples\n",
      "[0.21085892617702484, 0.13861456513404846, 0.015492390841245651] → 1 samples\n",
      "[0.211494579911232, 0.09739582240581512, 0.2421167939901352] → 1 samples\n",
      "[0.21364004909992218, 0.1832394003868103, 0.37733685970306396] → 1 samples\n",
      "[0.21446600556373596, 0.08122770488262177, 0.7643905878067017] → 1 samples\n",
      "[0.21524466574192047, 0.04711432009935379, 0.3703274726867676] → 1 samples\n",
      "[0.2163994163274765, 0.22980213165283203, 0.07078705728054047] → 1 samples\n",
      "[0.21651317179203033, 0.12945669889450073, 0.07924842089414597] → 1 samples\n",
      "[0.21708877384662628, 0.12084701657295227, 0.36603736877441406] → 1 samples\n",
      "[0.21889714896678925, 0.13845427334308624, 0.03604236990213394] → 1 samples\n",
      "[0.22003208100795746, 0.09293903410434723, 0.37884265184402466] → 1 samples\n",
      "[0.22077034413814545, 0.2357315570116043, 0.059514522552490234] → 1 samples\n",
      "[0.22144493460655212, 0.1049836277961731, 0.03470485284924507] → 1 samples\n",
      "[0.22650384902954102, 0.18541717529296875, 0.23802635073661804] → 1 samples\n",
      "[0.22671420872211456, 0.10438530892133713, 0.0768091157078743] → 1 samples\n",
      "[0.22919824719429016, 0.1689101606607437, 0.03786049783229828] → 1 samples\n",
      "[0.2300620824098587, 0.1372552365064621, 0.07400012016296387] → 1 samples\n",
      "[0.23020939528942108, 0.06745564937591553, 0.030568962916731834] → 1 samples\n",
      "[0.23066383600234985, 0.1006152331829071, 0.07592136412858963] → 1 samples\n",
      "[0.23131583631038666, 0.28177541494369507, 0.1137026697397232] → 1 samples\n",
      "[0.23233796656131744, 0.08750315010547638, 0.9430525302886963] → 1 samples\n",
      "[0.2331465780735016, 0.06179095432162285, 0.0408891923725605] → 1 samples\n",
      "[0.23440442979335785, 0.15382619202136993, 0.039482735097408295] → 1 samples\n",
      "[0.23499274253845215, 0.08324769139289856, 0.004620515741407871] → 1 samples\n",
      "[0.23513944447040558, 0.11757762730121613, 0.06787373125553131] → 1 samples\n",
      "[0.23651930689811707, 0.22150234878063202, 0.21141394972801208] → 1 samples\n",
      "[0.23734161257743835, 0.07425262778997421, 0.08034607768058777] → 1 samples\n",
      "[0.23922237753868103, 0.2714720070362091, 0.3601478040218353] → 1 samples\n",
      "[0.23970913887023926, 0.1674753874540329, 0.46493926644325256] → 1 samples\n",
      "[0.24020333588123322, 0.06925829499959946, 0.1270788162946701] → 1 samples\n",
      "[0.24036043882369995, 0.033877987414598465, 0.2674524188041687] → 1 samples\n",
      "[0.2414761781692505, 0.08010222017765045, 0.03997283801436424] → 1 samples\n",
      "[0.24301555752754211, 0.10642547905445099, 0.028754781931638718] → 1 samples\n",
      "[0.2439526915550232, 0.368812620639801, 0.18710586428642273] → 1 samples\n",
      "[0.2456786185503006, 0.17853648960590363, 0.06679569184780121] → 1 samples\n",
      "[0.24662162363529205, 0.1729496419429779, 0.026253288611769676] → 1 samples\n",
      "[0.24663612246513367, 0.30514490604400635, 0.05087413638830185] → 1 samples\n",
      "[0.24755749106407166, 0.21053913235664368, 0.14678357541561127] → 1 samples\n",
      "[0.2495701164007187, 0.056062936782836914, 0.1626700460910797] → 1 samples\n",
      "[0.249684140086174, 0.2180963009595871, 0.14369399845600128] → 1 samples\n",
      "[0.25002121925354004, 0.17492303252220154, 0.01652592048048973] → 1 samples\n",
      "[0.25174570083618164, 0.2771736979484558, 0.1683150976896286] → 1 samples\n",
      "[0.25282517075538635, 0.18477486073970795, 0.1911710947751999] → 1 samples\n",
      "[0.25359904766082764, 0.05730172619223595, 0.1480594426393509] → 1 samples\n",
      "[0.25550222396850586, 0.07751452177762985, 0.01638895273208618] → 1 samples\n",
      "[0.2558523118495941, 0.19337035715579987, 0.029718872159719467] → 1 samples\n",
      "[0.25681009888648987, 0.15821748971939087, 0.19372373819351196] → 1 samples\n",
      "[0.2581964135169983, 0.21583357453346252, 0.008817886933684349] → 1 samples\n",
      "[0.2587597370147705, 0.04892871528863907, 0.474849671125412] → 1 samples\n",
      "[0.25889554619789124, 0.12066596746444702, 0.12856493890285492] → 1 samples\n",
      "[0.25914502143859863, 0.055975694209337234, 0.13743817806243896] → 1 samples\n",
      "[0.2595617175102234, 0.21498312056064606, 0.8170098066329956] → 1 samples\n",
      "[0.25989681482315063, 0.1338331699371338, 0.15964585542678833] → 1 samples\n",
      "[0.2605637311935425, 0.19972467422485352, 0.05752868577837944] → 1 samples\n",
      "[0.26056939363479614, 0.20362797379493713, 0.07474279403686523] → 1 samples\n",
      "[0.26060229539871216, 0.0971798226237297, 0.09134911745786667] → 1 samples\n",
      "[0.2613556683063507, 0.06508627533912659, 0.39654865860939026] → 1 samples\n",
      "[0.26216068863868713, 0.0692354068160057, 0.047124918550252914] → 1 samples\n",
      "[0.2631182372570038, 0.07840181142091751, 0.12842720746994019] → 1 samples\n",
      "[0.26545897126197815, 0.14655938744544983, 0.3458487391471863] → 1 samples\n",
      "[0.26546987891197205, 0.15748122334480286, 0.28660398721694946] → 1 samples\n",
      "[0.2659670412540436, 0.13858601450920105, 0.2780858874320984] → 1 samples\n",
      "[0.2662809193134308, 0.21390587091445923, 0.603679358959198] → 1 samples\n",
      "[0.26677045226097107, 0.12194984406232834, 0.14325197041034698] → 1 samples\n",
      "[0.2670604884624481, 0.3810442388057709, 0.651875376701355] → 1 samples\n",
      "[0.2670985758304596, 0.05014951899647713, 0.05988604202866554] → 1 samples\n",
      "[0.2674078643321991, 0.05789223313331604, 0.23959876596927643] → 1 samples\n",
      "[0.2674645781517029, 0.15088623762130737, 0.28231167793273926] → 1 samples\n",
      "[0.2681705057621002, 0.30174216628074646, 0.5294478535652161] → 1 samples\n",
      "[0.2682670056819916, 0.21326352655887604, 0.03798690065741539] → 1 samples\n",
      "[0.2700764238834381, 0.17429542541503906, 0.34545671939849854] → 1 samples\n",
      "[0.27024900913238525, 0.0760272815823555, 0.05546675622463226] → 1 samples\n",
      "[0.270526647567749, 0.12595315277576447, 0.20576529204845428] → 1 samples\n",
      "[0.27101925015449524, 0.0741751566529274, 0.0730477049946785] → 1 samples\n",
      "[0.2710295021533966, 0.199818953871727, 0.49652114510536194] → 1 samples\n",
      "[0.2711331248283386, 0.26291853189468384, 0.7366147041320801] → 1 samples\n",
      "[0.27162814140319824, 0.10148696601390839, 0.04538782685995102] → 1 samples\n",
      "[0.27222102880477905, 0.17601792514324188, 0.19783207774162292] → 1 samples\n",
      "[0.27249789237976074, 0.20314761996269226, 0.6934220790863037] → 1 samples\n",
      "[0.27303096652030945, 0.11789338290691376, 0.053206998854875565] → 1 samples\n",
      "[0.2739473283290863, 0.061586927622556686, 0.012216818518936634] → 1 samples\n",
      "[0.2744808495044708, 0.12631070613861084, 0.20734521746635437] → 1 samples\n",
      "[0.27450674772262573, 0.07301323115825653, 0.16369199752807617] → 1 samples\n",
      "[0.27501729130744934, 0.18159990012645721, 0.23870237171649933] → 1 samples\n",
      "[0.27586567401885986, 0.1737332046031952, 0.1515156775712967] → 1 samples\n",
      "[0.2762310206890106, 0.1546119600534439, 0.14718860387802124] → 1 samples\n",
      "[0.2782818078994751, 0.09836435317993164, 0.08353520929813385] → 1 samples\n",
      "[0.27878522872924805, 0.33358511328697205, 0.09735330939292908] → 1 samples\n",
      "[0.2795228958129883, 0.1256651133298874, 0.035022247582674026] → 1 samples\n",
      "[0.27969324588775635, 0.1728556752204895, 0.17109431326389313] → 1 samples\n",
      "[0.28013259172439575, 0.1612202674150467, 0.07402230054140091] → 1 samples\n",
      "[0.28017348051071167, 0.16180385649204254, 0.0693918839097023] → 1 samples\n",
      "[0.280251681804657, 0.12803365290164948, 0.17685140669345856] → 1 samples\n",
      "[0.28105393052101135, 0.13021916151046753, 0.08863014727830887] → 1 samples\n",
      "[0.28133007884025574, 0.14823922514915466, 0.09128692001104355] → 1 samples\n",
      "[0.2815477252006531, 0.18116967380046844, 0.18188665807247162] → 1 samples\n",
      "[0.2818145453929901, 0.1850498914718628, 0.5407873392105103] → 1 samples\n",
      "[0.28183257579803467, 0.11014234274625778, 0.008183891884982586] → 1 samples\n",
      "[0.28240901231765747, 0.19910983741283417, 0.473514586687088] → 1 samples\n",
      "[0.2825537621974945, 0.1062467098236084, 0.0385095588862896] → 1 samples\n",
      "[0.28293395042419434, 0.15560075640678406, 0.011389208026230335] → 1 samples\n",
      "[0.2831886410713196, 0.09190010279417038, 0.08985210210084915] → 1 samples\n",
      "[0.2832520008087158, 0.18650972843170166, 0.03371712192893028] → 1 samples\n",
      "[0.2832600772380829, 0.16712169349193573, 0.3815276622772217] → 1 samples\n",
      "[0.28328853845596313, 0.08411860466003418, 0.2663101851940155] → 1 samples\n",
      "[0.2833857536315918, 0.10713135451078415, 0.05086544528603554] → 1 samples\n",
      "[0.2838236093521118, 0.16001008450984955, 0.22656939923763275] → 1 samples\n",
      "[0.28386449813842773, 0.09821265190839767, 0.1742037534713745] → 1 samples\n",
      "[0.28394150733947754, 0.1403084695339203, 0.3018796443939209] → 1 samples\n",
      "[0.2840566635131836, 0.4290897250175476, 0.2023882269859314] → 1 samples\n",
      "[0.2841053605079651, 0.24143968522548676, 0.12892504036426544] → 1 samples\n",
      "[0.28422799706459045, 0.20241284370422363, 0.12805518507957458] → 1 samples\n",
      "[0.2843672037124634, 0.14936047792434692, 0.024221017956733704] → 1 samples\n",
      "[0.28449156880378723, 0.1418560892343521, 0.20726704597473145] → 1 samples\n",
      "[0.2845436632633209, 0.13437645137310028, 0.0426628515124321] → 1 samples\n",
      "[0.2849026024341583, 0.08913430571556091, 0.03217378258705139] → 1 samples\n",
      "[0.2849627137184143, 0.1899183839559555, 0.41393449902534485] → 1 samples\n",
      "[0.2853836417198181, 0.18276728689670563, 0.11757896095514297] → 1 samples\n",
      "[0.2855566442012787, 0.1549602895975113, 0.11394164711236954] → 1 samples\n",
      "[0.2856491804122925, 0.18165923655033112, 0.02479538321495056] → 1 samples\n",
      "[0.2861027419567108, 0.15791821479797363, 0.2750461995601654] → 1 samples\n",
      "[0.2862226963043213, 0.3131817877292633, 0.10845678299665451] → 1 samples\n",
      "[0.28641223907470703, 0.14034053683280945, 0.381632924079895] → 1 samples\n",
      "[0.2864767611026764, 0.4820137321949005, 0.16446445882320404] → 1 samples\n",
      "[0.2865137457847595, 0.11711146682500839, 0.14493316411972046] → 1 samples\n",
      "[0.2865684926509857, 0.1346537172794342, 0.01149978768080473] → 1 samples\n",
      "[0.2868032455444336, 0.30806636810302734, 0.2095049023628235] → 1 samples\n",
      "[0.28696975111961365, 0.2479555606842041, 0.052532993257045746] → 1 samples\n",
      "[0.2872084677219391, 0.13298645615577698, 0.2637102007865906] → 1 samples\n",
      "[0.28721335530281067, 0.13133305311203003, 0.05461105331778526] → 1 samples\n",
      "[0.28722119331359863, 0.14266790449619293, 0.07498679310083389] → 1 samples\n",
      "[0.28738540410995483, 0.22857092320919037, 0.052351199090480804] → 1 samples\n",
      "[0.28747689723968506, 0.1607566624879837, 0.07738762348890305] → 1 samples\n",
      "[0.2875717878341675, 0.3545311391353607, 0.24952970445156097] → 1 samples\n",
      "[0.2882711589336395, 0.12505029141902924, 0.12145251035690308] → 1 samples\n",
      "[0.2886688709259033, 0.23549236357212067, 0.5068725943565369] → 1 samples\n",
      "[0.2889029383659363, 0.26442569494247437, 0.06007757782936096] → 1 samples\n",
      "[0.2889275848865509, 0.20137476921081543, 0.3368312120437622] → 1 samples\n",
      "[0.2890598475933075, 0.11559674143791199, 0.20837578177452087] → 1 samples\n",
      "[0.28956717252731323, 0.13205565512180328, 0.05103287100791931] → 1 samples\n",
      "[0.28967607021331787, 0.1365116983652115, 0.7130982279777527] → 1 samples\n",
      "[0.28972217440605164, 0.16770081222057343, 0.16880081593990326] → 1 samples\n",
      "[0.28982752561569214, 0.12132932990789413, 0.0393170490860939] → 1 samples\n",
      "[0.2902350127696991, 0.18339519202709198, 0.12747126817703247] → 1 samples\n",
      "[0.29035645723342896, 0.17445982992649078, 0.05609014257788658] → 1 samples\n",
      "[0.2910354733467102, 0.21144239604473114, 0.5714336037635803] → 1 samples\n",
      "[0.2914887070655823, 0.1408408284187317, 0.28626587986946106] → 1 samples\n",
      "[0.2916219234466553, 0.23045630753040314, 0.270995169878006] → 1 samples\n",
      "[0.29162198305130005, 0.1980188488960266, 0.15016649663448334] → 1 samples\n",
      "[0.2917689383029938, 0.1971924901008606, 0.027400076389312744] → 1 samples\n",
      "[0.2917987108230591, 0.17105115950107574, 0.03181714937090874] → 1 samples\n",
      "[0.2919253408908844, 0.12432201951742172, 0.09935557097196579] → 1 samples\n",
      "[0.29205450415611267, 0.1688075065612793, 0.12599298357963562] → 1 samples\n",
      "[0.29224634170532227, 0.13757732510566711, 0.25177446007728577] → 1 samples\n",
      "[0.29226988554000854, 0.10256247222423553, 0.1932377964258194] → 1 samples\n",
      "[0.29265865683555603, 0.18995770812034607, 0.08036846667528152] → 1 samples\n",
      "[0.29301226139068604, 0.12637771666049957, 0.02637859806418419] → 1 samples\n",
      "[0.2932620644569397, 0.10521771758794785, 0.28628355264663696] → 1 samples\n",
      "[0.29362404346466064, 0.2244766354560852, 0.19366246461868286] → 1 samples\n",
      "[0.2940804660320282, 0.14157021045684814, 0.07444547116756439] → 1 samples\n",
      "[0.29421740770339966, 0.1206820085644722, 0.047812897711992264] → 1 samples\n",
      "[0.2942664921283722, 0.1902286559343338, 0.03536803275346756] → 1 samples\n",
      "[0.29439792037010193, 0.3306024968624115, 0.07908771187067032] → 1 samples\n",
      "[0.29444238543510437, 0.12210503965616226, 0.29718145728111267] → 1 samples\n",
      "[0.2946091890335083, 0.14431296288967133, 0.2295316755771637] → 1 samples\n",
      "[0.2946232259273529, 0.16305366158485413, 0.15637728571891785] → 1 samples\n",
      "[0.2950765788555145, 0.09017340838909149, 0.08977368474006653] → 1 samples\n",
      "[0.2954365909099579, 0.11043105274438858, 0.04729094356298447] → 1 samples\n",
      "[0.2955513000488281, 0.18174059689044952, 0.2725350558757782] → 1 samples\n",
      "[0.2959868013858795, 0.2738197445869446, 0.008072958327829838] → 1 samples\n",
      "[0.2967979311943054, 0.14879438281059265, 0.07873678207397461] → 1 samples\n",
      "[0.2969048321247101, 0.14864395558834076, 0.2892005741596222] → 1 samples\n",
      "[0.2969810664653778, 0.21754957735538483, 0.5259665846824646] → 1 samples\n",
      "[0.2970120310783386, 0.18159496784210205, 0.13134175539016724] → 1 samples\n",
      "[0.29753878712654114, 0.46299371123313904, 0.07985137403011322] → 1 samples\n",
      "[0.2975730001926422, 0.29108884930610657, 0.08092053234577179] → 1 samples\n",
      "[0.2982659339904785, 0.131730318069458, 0.17579320073127747] → 1 samples\n",
      "[0.2982814610004425, 0.26384294033050537, 0.08969809114933014] → 1 samples\n",
      "[0.29898279905319214, 0.13005682826042175, 0.36750200390815735] → 1 samples\n",
      "[0.2989922761917114, 0.1345238834619522, 0.10306642949581146] → 1 samples\n",
      "[0.29935339093208313, 0.21749266982078552, 0.8412943482398987] → 1 samples\n",
      "[0.29945141077041626, 0.1655813753604889, 0.8964019417762756] → 1 samples\n",
      "[0.29952874779701233, 0.11751040071249008, 0.08626207709312439] → 1 samples\n",
      "[0.2995757460594177, 0.11100649833679199, 0.10864716023206711] → 1 samples\n",
      "[0.29977700114250183, 0.14860360324382782, 0.06409114599227905] → 1 samples\n",
      "[0.2998974919319153, 0.2014477699995041, 0.06515742838382721] → 1 samples\n",
      "[0.30007344484329224, 0.23874099552631378, 0.30898284912109375] → 1 samples\n",
      "[0.3001916706562042, 0.2689284682273865, 0.26831313967704773] → 1 samples\n",
      "[0.30038708448410034, 0.13737188279628754, 0.04991705343127251] → 1 samples\n",
      "[0.3003882169723511, 0.26139721274375916, 0.09626699239015579] → 1 samples\n",
      "[0.3005620241165161, 0.14825119078159332, 0.20286579430103302] → 1 samples\n",
      "[0.30057674646377563, 0.23053649067878723, 0.12977305054664612] → 1 samples\n",
      "[0.3006727993488312, 0.1588396579027176, 0.058921732008457184] → 1 samples\n",
      "[0.30074381828308105, 0.10712780803442001, 0.010777601040899754] → 1 samples\n",
      "[0.3007600009441376, 0.20963628590106964, 0.19613517820835114] → 1 samples\n",
      "[0.3008231520652771, 0.25238174200057983, 0.1383713036775589] → 1 samples\n",
      "[0.3011336624622345, 0.18965677917003632, 0.5486469864845276] → 1 samples\n",
      "[0.30113932490348816, 0.07991214841604233, 0.20255495607852936] → 1 samples\n",
      "[0.30132344365119934, 0.21788235008716583, 0.21213869750499725] → 1 samples\n",
      "[0.30152133107185364, 0.15062075853347778, 0.2632054090499878] → 1 samples\n",
      "[0.3016483187675476, 0.18192237615585327, 0.040978673845529556] → 1 samples\n",
      "[0.3017028868198395, 0.1451757848262787, 0.0504472479224205] → 1 samples\n",
      "[0.30205395817756653, 0.16435417532920837, 0.1304316222667694] → 1 samples\n",
      "[0.30206412076950073, 0.13387064635753632, 0.17639780044555664] → 1 samples\n",
      "[0.3024575710296631, 0.2185772955417633, 0.6942341923713684] → 1 samples\n",
      "[0.30264052748680115, 0.17594432830810547, 0.13609051704406738] → 1 samples\n",
      "[0.30269965529441833, 0.2660534381866455, 0.3270166218280792] → 1 samples\n",
      "[0.3027934730052948, 0.1249719113111496, 0.1974288523197174] → 1 samples\n",
      "[0.30318909883499146, 0.1475684940814972, 0.7065686583518982] → 1 samples\n",
      "[0.30319204926490784, 0.23687155544757843, 0.47536367177963257] → 1 samples\n",
      "[0.30319714546203613, 0.12227515131235123, 0.026129892095923424] → 1 samples\n",
      "[0.3032125234603882, 0.19850443303585052, 0.04503941908478737] → 1 samples\n",
      "[0.3032301366329193, 0.11544360965490341, 0.1392996460199356] → 1 samples\n",
      "[0.3034932613372803, 0.11586600542068481, 0.20878450572490692] → 1 samples\n",
      "[0.303803026676178, 0.15949058532714844, 0.16375592350959778] → 1 samples\n",
      "[0.30381232500076294, 0.12496720999479294, 0.014243626967072487] → 1 samples\n",
      "[0.30407118797302246, 0.13103951513767242, 0.16325612366199493] → 1 samples\n",
      "[0.30429041385650635, 0.20548437535762787, 0.2991759777069092] → 1 samples\n",
      "[0.30460599064826965, 0.2854343354701996, 0.5883572697639465] → 1 samples\n",
      "[0.3046908676624298, 0.15789327025413513, 0.39732858538627625] → 1 samples\n",
      "[0.30488306283950806, 0.14122410118579865, 0.20713630318641663] → 1 samples\n",
      "[0.30491530895233154, 0.09557855129241943, 0.16024918854236603] → 1 samples\n",
      "[0.30502113699913025, 0.23086121678352356, 0.42740654945373535] → 1 samples\n",
      "[0.3052540421485901, 0.2830200493335724, 0.42174071073532104] → 1 samples\n",
      "[0.3054138422012329, 0.1531139612197876, 0.043536264449357986] → 1 samples\n",
      "[0.3058774173259735, 0.18152295053005219, 0.29286307096481323] → 1 samples\n",
      "[0.30590200424194336, 0.22910475730895996, 0.6179805397987366] → 1 samples\n",
      "[0.30590224266052246, 0.19924500584602356, 0.08301172405481339] → 1 samples\n",
      "[0.3059484362602234, 0.26777052879333496, 0.1134161502122879] → 1 samples\n",
      "[0.3060585856437683, 0.19913092255592346, 0.430183470249176] → 1 samples\n",
      "[0.30638447403907776, 0.25878486037254333, 0.44026437401771545] → 1 samples\n",
      "[0.3064965605735779, 0.11259470880031586, 0.7357897758483887] → 1 samples\n",
      "[0.30661019682884216, 0.21799254417419434, 0.13501717150211334] → 1 samples\n",
      "[0.3070373237133026, 0.1119588240981102, 0.2764461934566498] → 1 samples\n",
      "[0.3070775866508484, 0.23181034624576569, 0.1875114142894745] → 1 samples\n",
      "[0.307391881942749, 0.2413617968559265, 0.6615375280380249] → 1 samples\n",
      "[0.30786067247390747, 0.14446453750133514, 0.12712781131267548] → 1 samples\n",
      "[0.3080236315727234, 0.2875516712665558, 0.14832621812820435] → 1 samples\n",
      "[0.3084677457809448, 0.22268685698509216, 0.3307797610759735] → 1 samples\n",
      "[0.30854493379592896, 0.1409997195005417, 0.31857746839523315] → 1 samples\n",
      "[0.30878645181655884, 0.25474652647972107, 0.3562730550765991] → 1 samples\n",
      "[0.30912330746650696, 0.10609453171491623, 0.16236074268817902] → 1 samples\n",
      "[0.30925512313842773, 0.2774484157562256, 0.4758460819721222] → 1 samples\n",
      "[0.309255987405777, 0.23334690928459167, 0.08317519724369049] → 1 samples\n",
      "[0.3093350827693939, 0.1652279645204544, 0.9314940571784973] → 1 samples\n",
      "[0.30936211347579956, 0.20391519367694855, 0.4032762348651886] → 1 samples\n",
      "[0.30946269631385803, 0.2918769419193268, 0.07804038375616074] → 1 samples\n",
      "[0.3095122277736664, 0.21760894358158112, 0.11382927745580673] → 1 samples\n",
      "[0.30989328026771545, 0.13881666958332062, 0.13457316160202026] → 1 samples\n",
      "[0.30997681617736816, 0.09570171684026718, 0.043851207941770554] → 1 samples\n",
      "[0.3100956678390503, 0.15307751297950745, 0.012724876403808594] → 1 samples\n",
      "[0.31052735447883606, 0.3181125819683075, 0.5013178586959839] → 1 samples\n",
      "[0.3109447658061981, 0.1735275685787201, 0.17775188386440277] → 1 samples\n",
      "[0.3111116588115692, 0.12799733877182007, 0.1801723688840866] → 1 samples\n",
      "[0.3113204538822174, 0.18337787687778473, 0.9111942052841187] → 1 samples\n",
      "[0.3113393187522888, 0.15384213626384735, 0.052258480340242386] → 1 samples\n",
      "[0.31185483932495117, 0.29800039529800415, 0.5949986577033997] → 1 samples\n",
      "[0.31211379170417786, 0.15025539696216583, 0.026649972423911095] → 1 samples\n",
      "[0.3124026954174042, 0.15837235748767853, 0.5730470418930054] → 1 samples\n",
      "[0.31312328577041626, 0.16819052398204803, 0.2097654640674591] → 1 samples\n",
      "[0.3131357431411743, 0.20220738649368286, 0.8467805981636047] → 1 samples\n",
      "[0.31322231888771057, 0.10698579996824265, 0.10906395316123962] → 1 samples\n",
      "[0.3133353590965271, 0.22521790862083435, 0.10044271498918533] → 1 samples\n",
      "[0.3133513033390045, 0.16335448622703552, 0.2925419807434082] → 1 samples\n",
      "[0.31356990337371826, 0.21499131619930267, 0.06839808821678162] → 1 samples\n",
      "[0.3137631416320801, 0.17976097762584686, 0.20410174131393433] → 1 samples\n",
      "[0.3138173818588257, 0.2761545479297638, 0.10790802538394928] → 1 samples\n",
      "[0.3141268193721771, 0.15067064762115479, 0.19656676054000854] → 1 samples\n",
      "[0.3141483664512634, 0.20334026217460632, 0.6431711316108704] → 1 samples\n",
      "[0.3145812749862671, 0.2580716609954834, 0.10440606623888016] → 1 samples\n",
      "[0.3147663474082947, 0.2686922550201416, 0.07727394253015518] → 1 samples\n",
      "[0.31496503949165344, 0.17997504770755768, 0.2939140796661377] → 1 samples\n",
      "[0.315172404050827, 0.11183732002973557, 0.00815251562744379] → 1 samples\n",
      "[0.31521403789520264, 0.24550744891166687, 0.42989081144332886] → 1 samples\n",
      "[0.31558069586753845, 0.1429349184036255, 0.016478966921567917] → 1 samples\n",
      "[0.3163259029388428, 0.12690290808677673, 0.6854143738746643] → 1 samples\n",
      "[0.3168552815914154, 0.16416199505329132, 0.17392727732658386] → 1 samples\n",
      "[0.31732064485549927, 0.126720130443573, 0.02656124159693718] → 1 samples\n",
      "[0.3193781077861786, 0.19727644324302673, 0.2087453305721283] → 1 samples\n",
      "[0.32045406103134155, 0.1912420094013214, 0.023241734132170677] → 1 samples\n",
      "[0.3204977512359619, 0.17148680984973907, 0.17123115062713623] → 1 samples\n",
      "[0.32094478607177734, 0.2906816005706787, 0.5396875143051147] → 1 samples\n",
      "[0.3212878704071045, 0.2044910192489624, 0.901729941368103] → 1 samples\n",
      "[0.32140401005744934, 0.21507370471954346, 0.06911654770374298] → 1 samples\n",
      "[0.3218267858028412, 0.25328969955444336, 0.419959157705307] → 1 samples\n",
      "[0.3218483030796051, 0.26353347301483154, 0.2156846672296524] → 1 samples\n",
      "[0.32203152775764465, 0.2877119183540344, 0.32638877630233765] → 1 samples\n",
      "[0.3221047818660736, 0.19671350717544556, 0.329689621925354] → 1 samples\n",
      "[0.3225504457950592, 0.3218693733215332, 0.7185498476028442] → 1 samples\n",
      "[0.3227265179157257, 0.09278813749551773, 0.17821510136127472] → 1 samples\n",
      "[0.32275477051734924, 0.2797762155532837, 0.015292216092348099] → 1 samples\n",
      "[0.32275646924972534, 0.26054418087005615, 0.2076578438282013] → 1 samples\n",
      "[0.3231033980846405, 0.26729801297187805, 0.48101261258125305] → 1 samples\n",
      "[0.32381829619407654, 0.3368317186832428, 0.1748725026845932] → 1 samples\n",
      "[0.3238927125930786, 0.2006392627954483, 0.6059120297431946] → 1 samples\n",
      "[0.3242325186729431, 0.17396777868270874, 0.11245613545179367] → 1 samples\n",
      "[0.32466715574264526, 0.3078206777572632, 0.4104698598384857] → 1 samples\n",
      "[0.32496970891952515, 0.2301093339920044, 0.08993831276893616] → 1 samples\n",
      "[0.32520943880081177, 0.16083763539791107, 0.51978999376297] → 1 samples\n",
      "[0.32567283511161804, 0.14630597829818726, 0.08045772463083267] → 1 samples\n",
      "[0.326341450214386, 0.29795941710472107, 0.22159357368946075] → 1 samples\n",
      "[0.32662054896354675, 0.2281470149755478, 0.408615380525589] → 1 samples\n",
      "[0.3280414044857025, 0.16571936011314392, 0.6821607947349548] → 1 samples\n",
      "[0.32839900255203247, 0.19076722860336304, 0.7793060541152954] → 1 samples\n",
      "[0.3296898305416107, 0.19115135073661804, 0.07900512963533401] → 1 samples\n",
      "[0.32996708154678345, 0.14188235998153687, 0.37306493520736694] → 1 samples\n",
      "[0.3306208550930023, 0.27027514576911926, 0.4307081401348114] → 1 samples\n",
      "[0.3307289779186249, 0.3535708785057068, 0.26805567741394043] → 1 samples\n",
      "[0.33078551292419434, 0.15946505963802338, 0.6643353700637817] → 1 samples\n",
      "[0.3309621214866638, 0.2110796421766281, 0.33893176913261414] → 1 samples\n",
      "[0.3315468430519104, 0.28951603174209595, 0.2755395174026489] → 1 samples\n",
      "[0.33181801438331604, 0.2520659863948822, 0.057810548692941666] → 1 samples\n",
      "[0.3320966064929962, 0.1778227686882019, 0.07826739549636841] → 1 samples\n",
      "[0.33210358023643494, 0.18616269528865814, 0.5358624458312988] → 1 samples\n",
      "[0.33224108815193176, 0.26868903636932373, 0.048676472157239914] → 1 samples\n",
      "[0.33244872093200684, 0.19512507319450378, 0.12232674658298492] → 1 samples\n",
      "[0.3325778543949127, 0.20996470749378204, 0.034866735339164734] → 1 samples\n",
      "[0.3332289457321167, 0.07266002893447876, 0.32229146361351013] → 1 samples\n",
      "[0.33342787623405457, 0.2480669617652893, 0.5192974805831909] → 1 samples\n",
      "[0.3334348499774933, 0.22823403775691986, 0.29757028818130493] → 1 samples\n",
      "[0.3348231613636017, 0.15959197282791138, 0.32357850670814514] → 1 samples\n",
      "[0.33505019545555115, 0.10562266409397125, 0.2106916308403015] → 1 samples\n",
      "[0.3355856239795685, 0.09498779475688934, 0.37415313720703125] → 1 samples\n",
      "[0.335786908864975, 0.2994832396507263, 0.4714745879173279] → 1 samples\n",
      "[0.33664897084236145, 0.31588104367256165, 0.03864365443587303] → 1 samples\n",
      "[0.3367559611797333, 0.15684902667999268, 0.3621087074279785] → 1 samples\n",
      "[0.337620347738266, 0.18997125327587128, 0.069893479347229] → 1 samples\n",
      "[0.3383162319660187, 0.38222628831863403, 0.08680124580860138] → 1 samples\n",
      "[0.338570773601532, 0.1929282248020172, 0.011147381737828255] → 1 samples\n",
      "[0.33866003155708313, 0.23302771151065826, 0.48542773723602295] → 1 samples\n",
      "[0.3397282660007477, 0.37668377161026, 0.17481309175491333] → 1 samples\n",
      "[0.3400000035762787, 0.24703973531723022, 0.5438928008079529] → 1 samples\n",
      "[0.34000450372695923, 0.0987134799361229, 0.04318190738558769] → 1 samples\n",
      "[0.3405890166759491, 0.2038968950510025, 0.10705550014972687] → 1 samples\n",
      "[0.3413775861263275, 0.25981390476226807, 0.06638678163290024] → 1 samples\n",
      "[0.3423580825328827, 0.17075763642787933, 0.15260742604732513] → 1 samples\n",
      "[0.3424582779407501, 0.20086300373077393, 0.49277564883232117] → 1 samples\n",
      "[0.34253472089767456, 0.2461787760257721, 0.08559004962444305] → 1 samples\n",
      "[0.3433934450149536, 0.21774859726428986, 0.14840653538703918] → 1 samples\n",
      "[0.34414365887641907, 0.3432852625846863, 0.9202308058738708] → 1 samples\n",
      "[0.34434017539024353, 0.21911099553108215, 0.40483391284942627] → 1 samples\n",
      "[0.34500548243522644, 0.17376387119293213, 0.036872051656246185] → 1 samples\n",
      "[0.3450087904930115, 0.2592974901199341, 0.029738739132881165] → 1 samples\n",
      "[0.3458680212497711, 0.3481212854385376, 0.35689404606819153] → 1 samples\n",
      "[0.3464454412460327, 0.3816821277141571, 0.21694141626358032] → 1 samples\n",
      "[0.3465840518474579, 0.23408907651901245, 0.09405460208654404] → 1 samples\n",
      "[0.3473055958747864, 0.1503746658563614, 0.05359301343560219] → 1 samples\n",
      "[0.3476385474205017, 0.19805790483951569, 0.1571822315454483] → 1 samples\n",
      "[0.3476618528366089, 0.3061736226081848, 0.5564603805541992] → 1 samples\n",
      "[0.347663551568985, 0.20229953527450562, 0.3587273061275482] → 1 samples\n",
      "[0.34785258769989014, 0.2760523855686188, 0.11680690944194794] → 1 samples\n",
      "[0.3492356538772583, 0.34050554037094116, 0.4539145231246948] → 1 samples\n",
      "[0.34942665696144104, 0.12212223559617996, 0.14987780153751373] → 1 samples\n",
      "[0.34988391399383545, 0.18747538328170776, 0.09120355546474457] → 1 samples\n",
      "[0.35002171993255615, 0.0670023038983345, 0.05142560601234436] → 1 samples\n",
      "[0.3501630127429962, 0.15023823082447052, 0.20456252992153168] → 1 samples\n",
      "[0.35039833188056946, 0.2774431109428406, 0.4476487338542938] → 1 samples\n",
      "[0.3507802486419678, 0.12303628027439117, 0.3486528694629669] → 1 samples\n",
      "[0.3525387644767761, 0.12774266302585602, 0.01660296320915222] → 1 samples\n",
      "[0.3525858223438263, 0.1834767460823059, 0.07399192452430725] → 1 samples\n",
      "[0.3527388572692871, 0.31540125608444214, 0.13836778700351715] → 1 samples\n",
      "[0.35418200492858887, 0.1253274530172348, 0.6701055765151978] → 1 samples\n",
      "[0.35445740818977356, 0.20411880314350128, 0.41122984886169434] → 1 samples\n",
      "[0.3548455834388733, 0.24798355996608734, 0.03049473464488983] → 1 samples\n",
      "[0.355129599571228, 0.22881202399730682, 0.19737431406974792] → 1 samples\n",
      "[0.3551486134529114, 0.2965479791164398, 0.18718937039375305] → 1 samples\n",
      "[0.35647517442703247, 0.20588621497154236, 0.6679333448410034] → 1 samples\n",
      "[0.35784289240837097, 0.2145768404006958, 0.22852709889411926] → 1 samples\n",
      "[0.35805457830429077, 0.1296510398387909, 0.24543194472789764] → 1 samples\n",
      "[0.3594893515110016, 0.20347602665424347, 0.032593246549367905] → 1 samples\n",
      "[0.3597716689109802, 0.272413969039917, 0.6484647393226624] → 1 samples\n",
      "[0.36025890707969666, 0.28387776017189026, 0.10065360367298126] → 1 samples\n",
      "[0.3602868914604187, 0.2662309408187866, 0.02755426988005638] → 1 samples\n",
      "[0.36049264669418335, 0.25712907314300537, 0.032381072640419006] → 1 samples\n",
      "[0.3616972267627716, 0.17054887115955353, 0.44596338272094727] → 1 samples\n",
      "[0.3621225357055664, 0.2486792802810669, 0.33639249205589294] → 1 samples\n",
      "[0.36278438568115234, 0.3936406075954437, 0.11480923742055893] → 1 samples\n",
      "[0.36280614137649536, 0.20640382170677185, 0.08629176020622253] → 1 samples\n",
      "[0.3639855980873108, 0.3325798213481903, 0.9145346283912659] → 1 samples\n",
      "[0.3640402853488922, 0.28911563754081726, 0.368338018655777] → 1 samples\n",
      "[0.36453011631965637, 0.32851603627204895, 0.6435108780860901] → 1 samples\n",
      "[0.36549824476242065, 0.1855747550725937, 0.5939406156539917] → 1 samples\n",
      "[0.36667582392692566, 0.15707628428936005, 0.047898806631565094] → 1 samples\n",
      "[0.36671188473701477, 0.33097681403160095, 0.7806977033615112] → 1 samples\n",
      "[0.3667607307434082, 0.19565731287002563, 0.024517418816685677] → 1 samples\n",
      "[0.3681959807872772, 0.19075661897659302, 0.010423068888485432] → 1 samples\n",
      "[0.36884626746177673, 0.2636330723762512, 0.3311445713043213] → 1 samples\n",
      "[0.3691346347332001, 0.1562764197587967, 0.004993923474103212] → 1 samples\n",
      "[0.3692021369934082, 0.22396327555179596, 0.023623555898666382] → 1 samples\n",
      "[0.3693777322769165, 0.15777191519737244, 0.4350535571575165] → 1 samples\n",
      "[0.36979180574417114, 0.1127958819270134, 0.8101397752761841] → 1 samples\n",
      "[0.37026092410087585, 0.12898248434066772, 0.03872771933674812] → 1 samples\n",
      "[0.3720951974391937, 0.1698375940322876, 0.8662883639335632] → 1 samples\n",
      "[0.37216970324516296, 0.2085951566696167, 0.05879035219550133] → 1 samples\n",
      "[0.373239129781723, 0.35557931661605835, 0.13460683822631836] → 1 samples\n",
      "[0.3740580379962921, 0.17150461673736572, 0.4523364007472992] → 1 samples\n",
      "[0.3751564919948578, 0.33872437477111816, 0.02119116112589836] → 1 samples\n",
      "[0.37516650557518005, 0.4286724925041199, 0.06789316236972809] → 1 samples\n",
      "[0.37527990341186523, 0.2079184353351593, 0.7409182190895081] → 1 samples\n",
      "[0.3758942484855652, 0.31349876523017883, 0.26859283447265625] → 1 samples\n",
      "[0.37598249316215515, 0.2014123946428299, 0.6880077719688416] → 1 samples\n",
      "[0.37626463174819946, 0.14176014065742493, 0.09262757003307343] → 1 samples\n",
      "[0.3772446811199188, 0.3548998534679413, 0.3150997459888458] → 1 samples\n",
      "[0.37786951661109924, 0.18356144428253174, 0.20866772532463074] → 1 samples\n",
      "[0.37851348519325256, 0.216705322265625, 0.4062456786632538] → 1 samples\n",
      "[0.37991097569465637, 0.39291912317276, 0.22598505020141602] → 1 samples\n",
      "[0.3810332119464874, 0.19456440210342407, 0.1236991211771965] → 1 samples\n",
      "[0.38107016682624817, 0.27692627906799316, 0.31510815024375916] → 1 samples\n",
      "[0.38153916597366333, 0.28735753893852234, 0.0740075409412384] → 1 samples\n",
      "[0.38207489252090454, 0.2854323983192444, 0.28793275356292725] → 1 samples\n",
      "[0.38251766562461853, 0.2368631362915039, 0.20111605525016785] → 1 samples\n",
      "[0.3825188875198364, 0.3211444318294525, 0.42407676577568054] → 1 samples\n",
      "[0.3833974301815033, 0.2516449987888336, 0.044988058507442474] → 1 samples\n",
      "[0.38340678811073303, 0.17745719850063324, 0.021536733955144882] → 1 samples\n",
      "[0.3848690986633301, 0.2660317122936249, 0.570141613483429] → 1 samples\n",
      "[0.3859998285770416, 0.1741867959499359, 0.35716184973716736] → 1 samples\n",
      "[0.3861629366874695, 0.17316313087940216, 0.10236769169569016] → 1 samples\n",
      "[0.38702622056007385, 0.2963208854198456, 0.33585697412490845] → 1 samples\n",
      "[0.387214720249176, 0.18659311532974243, 0.40804508328437805] → 1 samples\n",
      "[0.38764992356300354, 0.27423709630966187, 0.08794181793928146] → 1 samples\n",
      "[0.3882049322128296, 0.27947553992271423, 0.011375348083674908] → 1 samples\n",
      "[0.38883447647094727, 0.14321765303611755, 0.035487134009599686] → 1 samples\n",
      "[0.3902217745780945, 0.3558502495288849, 0.42775556445121765] → 1 samples\n",
      "[0.390892893075943, 0.2051338404417038, 0.04154498502612114] → 1 samples\n",
      "[0.39117148518562317, 0.4928518831729889, 0.8595861196517944] → 1 samples\n",
      "[0.3922325074672699, 0.4260249137878418, 0.8891159296035767] → 1 samples\n",
      "[0.3959178328514099, 0.2481977790594101, 0.0977814719080925] → 1 samples\n",
      "[0.3977712094783783, 0.2678455412387848, 0.2376025915145874] → 1 samples\n",
      "[0.39876076579093933, 0.15450194478034973, 0.08302300423383713] → 1 samples\n",
      "[0.39917847514152527, 0.2893162667751312, 0.036243267357349396] → 1 samples\n",
      "[0.39925724267959595, 0.2604403495788574, 0.027092469856142998] → 1 samples\n",
      "[0.39985543489456177, 0.14298398792743683, 0.34369754791259766] → 1 samples\n",
      "[0.40088504552841187, 0.27430692315101624, 0.020786195993423462] → 1 samples\n",
      "[0.40089112520217896, 0.34502115845680237, 0.14320950210094452] → 1 samples\n",
      "[0.4012534022331238, 0.2958050072193146, 0.23933301866054535] → 1 samples\n",
      "[0.40137264132499695, 0.29997262358665466, 0.03967714682221413] → 1 samples\n",
      "[0.4014514684677124, 0.2139045000076294, 0.7864312529563904] → 1 samples\n",
      "[0.4026646912097931, 0.19303007423877716, 0.04979000613093376] → 1 samples\n",
      "[0.4035489559173584, 0.21594470739364624, 0.07812300324440002] → 1 samples\n",
      "[0.4047970175743103, 0.3247663974761963, 0.6197391748428345] → 1 samples\n",
      "[0.40540504455566406, 0.42887794971466064, 0.38583633303642273] → 1 samples\n",
      "[0.40601229667663574, 0.4219500720500946, 0.34214863181114197] → 1 samples\n",
      "[0.4071354568004608, 0.2239234745502472, 0.01506601832807064] → 1 samples\n",
      "[0.4076521396636963, 0.22289903461933136, 0.823964536190033] → 1 samples\n",
      "[0.40989843010902405, 0.5050830245018005, 0.6824782490730286] → 1 samples\n",
      "[0.41573336720466614, 0.3866744935512543, 0.1342540979385376] → 1 samples\n",
      "[0.4167799651622772, 0.2075337916612625, 0.04064558446407318] → 1 samples\n",
      "[0.4179312288761139, 0.4585675001144409, 0.4180040657520294] → 1 samples\n",
      "[0.41844964027404785, 0.27904972434043884, 0.470769464969635] → 1 samples\n",
      "[0.41949376463890076, 0.23423637449741364, 0.2523132264614105] → 1 samples\n",
      "[0.42099958658218384, 0.2352447658777237, 0.554526150226593] → 1 samples\n",
      "[0.4276658892631531, 0.1818397045135498, 0.2344086766242981] → 1 samples\n",
      "[0.4279002845287323, 0.3548484742641449, 0.02080029621720314] → 1 samples\n",
      "[0.42919406294822693, 0.450876384973526, 0.957219660282135] → 1 samples\n",
      "[0.43080925941467285, 0.16351917386054993, 0.03518851473927498] → 1 samples\n",
      "[0.4308585822582245, 0.23196329176425934, 0.6632387638092041] → 1 samples\n",
      "[0.4347507953643799, 0.2651809751987457, 0.39283037185668945] → 1 samples\n",
      "[0.4348253905773163, 0.09600300341844559, 0.33553943037986755] → 1 samples\n",
      "[0.43586599826812744, 0.196288600564003, 0.37618643045425415] → 1 samples\n",
      "[0.4373352825641632, 0.42297670245170593, 0.2293735146522522] → 1 samples\n",
      "[0.4375078082084656, 0.3438320755958557, 0.2995392382144928] → 1 samples\n",
      "[0.43848904967308044, 0.2642115354537964, 0.25347521901130676] → 1 samples\n",
      "[0.4385869801044464, 0.254476934671402, 0.330120712518692] → 1 samples\n",
      "[0.44147464632987976, 0.3722066283226013, 0.8251530528068542] → 1 samples\n",
      "[0.4425674378871918, 0.17688137292861938, 0.47009286284446716] → 1 samples\n",
      "[0.4437900483608246, 0.35295000672340393, 0.3204460144042969] → 1 samples\n",
      "[0.4463628828525543, 0.254721462726593, 0.6880348324775696] → 1 samples\n",
      "[0.4515475928783417, 0.36322879791259766, 0.8768033981323242] → 1 samples\n",
      "[0.45459601283073425, 0.4354597330093384, 0.11236553639173508] → 1 samples\n",
      "[0.454746276140213, 0.3563123643398285, 0.12323766201734543] → 1 samples\n",
      "[0.45522984862327576, 0.1344861388206482, 0.024224938824772835] → 1 samples\n",
      "[0.4555879831314087, 0.31392818689346313, 0.04425055906176567] → 1 samples\n",
      "[0.45559224486351013, 0.26045629382133484, 0.2924920916557312] → 1 samples\n",
      "[0.4563397467136383, 0.27671563625335693, 0.6600775718688965] → 1 samples\n",
      "[0.4568992257118225, 0.2160712331533432, 0.2222522348165512] → 1 samples\n",
      "[0.45857349038124084, 0.2632482051849365, 0.8573343753814697] → 1 samples\n",
      "[0.4631127715110779, 0.39382404088974, 0.011391903273761272] → 1 samples\n",
      "[0.46645092964172363, 0.497464120388031, 0.39963629841804504] → 1 samples\n",
      "[0.46652835607528687, 0.38630908727645874, 0.7067657709121704] → 1 samples\n",
      "[0.4665337800979614, 0.4579100012779236, 0.26190921664237976] → 1 samples\n",
      "[0.4684394299983978, 0.3886391222476959, 0.05967480316758156] → 1 samples\n",
      "[0.4702858328819275, 0.49300894141197205, 0.37113747000694275] → 1 samples\n",
      "[0.47600460052490234, 0.4483804702758789, 0.6498197913169861] → 1 samples\n",
      "[0.4817346930503845, 0.34042683243751526, 0.8959397673606873] → 1 samples\n",
      "[0.48728039860725403, 0.5195333361625671, 0.4956454634666443] → 1 samples\n",
      "[0.48828378319740295, 0.42927393317222595, 0.1562637835741043] → 1 samples\n",
      "[0.4894280433654785, 0.25606104731559753, 0.043023981153964996] → 1 samples\n",
      "[0.49023985862731934, 0.36885884404182434, 0.33864378929138184] → 1 samples\n",
      "[0.4912610948085785, 0.4079453647136688, 0.3800206780433655] → 1 samples\n",
      "[0.49377205967903137, 0.3124227225780487, 0.04856108874082565] → 1 samples\n",
      "[0.49497929215431213, 0.19762325286865234, 0.00933157466351986] → 1 samples\n",
      "[0.4951435625553131, 0.43687883019447327, 0.058312442153692245] → 1 samples\n",
      "[0.49540191888809204, 0.24625517427921295, 0.38551419973373413] → 1 samples\n",
      "[0.4995872378349304, 0.35726025700569153, 0.21195201575756073] → 1 samples\n",
      "[0.503385603427887, 0.4533112347126007, 0.06792657822370529] → 1 samples\n",
      "[0.508388102054596, 0.23132432997226715, 0.14714767038822174] → 1 samples\n",
      "[0.5086661577224731, 0.46178728342056274, 0.08365311473608017] → 1 samples\n",
      "[0.5089607834815979, 0.445512980222702, 0.34433743357658386] → 1 samples\n",
      "[0.5175344944000244, 0.20281808078289032, 0.1027257964015007] → 1 samples\n",
      "[0.5198015570640564, 0.31275004148483276, 0.1748020201921463] → 1 samples\n",
      "[0.5240370631217957, 0.31464633345603943, 0.21820120513439178] → 1 samples\n",
      "[0.5296462774276733, 0.3801667094230652, 0.15244030952453613] → 1 samples\n",
      "[0.531341016292572, 0.4227586090564728, 0.03910225257277489] → 1 samples\n",
      "[0.538506269454956, 0.4220027029514313, 0.5770790576934814] → 1 samples\n",
      "[0.5402722358703613, 0.29875344038009644, 0.317698210477829] → 1 samples\n",
      "[0.5442436337471008, 0.15431155264377594, 0.022331010550260544] → 1 samples\n",
      "[0.5567753314971924, 0.2647709548473358, 0.38843703269958496] → 1 samples\n",
      "[0.561051607131958, 0.4477735757827759, 0.23572012782096863] → 1 samples\n",
      "[0.5649510622024536, 0.46921440958976746, 0.2624436616897583] → 1 samples\n",
      "[0.5659022927284241, 0.3401675820350647, 0.8463958501815796] → 1 samples\n",
      "[0.567425012588501, 0.46784508228302, 0.29986053705215454] → 1 samples\n",
      "[0.5717453360557556, 0.4847544729709625, 0.20099125802516937] → 1 samples\n",
      "[0.5723404288291931, 0.32970836758613586, 0.15342074632644653] → 1 samples\n",
      "[0.5753530263900757, 0.4982602596282959, 0.026603082194924355] → 1 samples\n",
      "[0.5758669376373291, 0.32241570949554443, 0.07055259495973587] → 1 samples\n",
      "[0.5769571661949158, 0.3401024341583252, 0.5768781304359436] → 1 samples\n",
      "[0.5821067690849304, 0.3858388364315033, 0.11682004481554031] → 1 samples\n",
      "[0.5834009647369385, 0.4442026615142822, 0.801560640335083] → 1 samples\n",
      "[0.5853286981582642, 0.30355700850486755, 0.17832709848880768] → 1 samples\n",
      "[0.5880729556083679, 0.38414502143859863, 0.10222902148962021] → 1 samples\n",
      "[0.5888250470161438, 0.377757728099823, 0.07346237450838089] → 1 samples\n",
      "[0.5892900228500366, 0.3957611620426178, 0.06688171625137329] → 1 samples\n",
      "[0.5986201763153076, 0.3778556287288666, 0.04325949773192406] → 1 samples\n",
      "[0.5999917984008789, 0.37742844223976135, 0.4025471806526184] → 1 samples\n",
      "[0.6019028425216675, 0.43362534046173096, 0.23741833865642548] → 1 samples\n",
      "[0.6022610664367676, 0.3488921821117401, 0.30852141976356506] → 1 samples\n",
      "[0.6029568910598755, 0.47146570682525635, 0.4169841408729553] → 1 samples\n",
      "[0.6104831695556641, 0.40228506922721863, 0.9569115042686462] → 1 samples\n",
      "[0.6106431484222412, 0.4234156012535095, 0.9165378212928772] → 1 samples\n",
      "[0.6110178828239441, 0.377412885427475, 0.21833492815494537] → 1 samples\n",
      "[0.6128702759742737, 0.30840978026390076, 0.3451596796512604] → 1 samples\n",
      "[0.6202437877655029, 0.5932035446166992, 0.047048408538103104] → 1 samples\n",
      "[0.6251161098480225, 0.40551483631134033, 0.4644980728626251] → 1 samples\n",
      "[0.6299228072166443, 0.4779548645019531, 0.23426134884357452] → 1 samples\n",
      "[0.6308109164237976, 0.42581868171691895, 0.8762790560722351] → 1 samples\n",
      "[0.6328621506690979, 0.39118069410324097, 0.10706614702939987] → 1 samples\n",
      "[0.6352054476737976, 0.4365007281303406, 0.9633032083511353] → 1 samples\n",
      "[0.6424720883369446, 0.5730260014533997, 0.862778902053833] → 1 samples\n",
      "[0.6537291407585144, 0.39107269048690796, 0.38192999362945557] → 1 samples\n",
      "[0.6599380373954773, 0.4088764190673828, 0.37305575609207153] → 1 samples\n",
      "[0.6599387526512146, 0.384257048368454, 0.08432423323392868] → 1 samples\n",
      "[0.6610831022262573, 0.5377567410469055, 0.7477251291275024] → 1 samples\n",
      "[0.6646305322647095, 0.2955693304538727, 0.17779876291751862] → 1 samples\n",
      "[0.6655821204185486, 0.397763729095459, 0.2841454744338989] → 1 samples\n",
      "[0.6694591045379639, 0.34090885519981384, 0.7854836583137512] → 1 samples\n",
      "[0.6801325082778931, 0.4811670780181885, 0.387361615896225] → 1 samples\n",
      "[0.6877765655517578, 0.5199365615844727, 0.42742183804512024] → 1 samples\n",
      "[0.6885170936584473, 0.42864659428596497, 0.04353663697838783] → 1 samples\n",
      "[0.6888831257820129, 0.5087395906448364, 0.6272693872451782] → 1 samples\n",
      "[0.6975237727165222, 0.44095876812934875, 0.9549460411071777] → 1 samples\n",
      "[0.7016187906265259, 0.540178656578064, 0.37294650077819824] → 1 samples\n",
      "[0.7027207612991333, 0.5081806182861328, 0.27044200897216797] → 1 samples\n",
      "[0.7079298496246338, 0.3824976682662964, 0.8111891746520996] → 1 samples\n",
      "[0.7081797122955322, 0.43044671416282654, 0.4213182032108307] → 1 samples\n",
      "[0.7157365083694458, 0.43106794357299805, 0.8906869292259216] → 1 samples\n",
      "[0.7181437015533447, 0.4595933258533478, 0.21033909916877747] → 1 samples\n",
      "[0.719845712184906, 0.5829252004623413, 0.5040153861045837] → 1 samples\n",
      "[0.7206374406814575, 0.49407845735549927, 0.9863203167915344] → 1 samples\n",
      "[0.7235621809959412, 0.4892656207084656, 0.819830060005188] → 1 samples\n",
      "[0.7271976470947266, 0.3680969774723053, 0.44918155670166016] → 1 samples\n",
      "[0.727910041809082, 0.4444599151611328, 0.08980558812618256] → 1 samples\n",
      "[0.73605877161026, 0.5159900188446045, 0.7570890784263611] → 1 samples\n",
      "[0.7369266748428345, 0.5340036749839783, 0.9605739712715149] → 1 samples\n",
      "[0.7494089603424072, 0.5365782976150513, 0.0280976053327322] → 1 samples\n",
      "[0.7517379522323608, 0.5178413391113281, 0.532099187374115] → 1 samples\n",
      "[0.7876119613647461, 0.44889208674430847, 0.06278114765882492] → 1 samples\n",
      "[0.8149020075798035, 0.4907011091709137, 0.03567017987370491] → 1 samples\n"
     ]
    }
   ],
   "source": [
    "# Checking that there is more than one unique row for final_preds\n",
    "uniq, counts = torch.unique(final_preds, return_counts=True, dim=0)\n",
    "for u,c in zip(uniq, counts):\n",
    "    print(u.tolist(), \"→\", c.item(), \"samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe3efe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a DataLoader for meta‐training\n",
    "meta_ds = TensorDataset(final_preds, final_labels)\n",
    "meta_loader= DataLoader(meta_ds, batch_size = 64, shuffle = True)\n",
    "\n",
    "# Defining the MetaModel (logistic regression)\n",
    "class MetaModel(nn.Module):\n",
    "    def __init__(self, in_features = 3, num_classes = 2): # 3 inputs (from 3 base models), 2 outputs for 2 classes\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "meta_model = MetaModel().to(device)\n",
    "\n",
    "# Optimizer & Loss\n",
    "optimizer = torch.optim.Adam(meta_model.parameters(), lr = 1e-2)\n",
    "neg, pos = (final_labels == 0).sum(), (final_labels == 1).sum()\n",
    "weights = torch.tensor([1.0, neg/pos]).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b12f90ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20  loss: 0.6983, acc: 71.59%  cls0_acc: 78.58%, cls1_acc: 24.05%\n",
      "Epoch 2/20  loss: 0.6777, acc: 41.72%  cls0_acc: 35.94%, cls1_acc: 81.01%\n",
      "Epoch 3/20  loss: 0.6525, acc: 44.81%  cls0_acc: 37.43%, cls1_acc: 94.94%\n",
      "Epoch 4/20  loss: 0.6340, acc: 62.34%  cls0_acc: 57.54%, cls1_acc: 94.94%\n",
      "Epoch 5/20  loss: 0.6151, acc: 72.08%  cls0_acc: 69.09%, cls1_acc: 92.41%\n",
      "Epoch 6/20  loss: 0.5960, acc: 77.76%  cls0_acc: 75.98%, cls1_acc: 89.87%\n",
      "Epoch 7/20  loss: 0.5815, acc: 79.55%  cls0_acc: 78.03%, cls1_acc: 89.87%\n",
      "Epoch 8/20  loss: 0.5654, acc: 79.55%  cls0_acc: 77.28%, cls1_acc: 94.94%\n",
      "Epoch 9/20  loss: 0.5477, acc: 80.52%  cls0_acc: 78.40%, cls1_acc: 94.94%\n",
      "Epoch 10/20  loss: 0.5374, acc: 82.79%  cls0_acc: 81.01%, cls1_acc: 94.94%\n",
      "Epoch 11/20  loss: 0.5231, acc: 84.58%  cls0_acc: 83.43%, cls1_acc: 92.41%\n",
      "Epoch 12/20  loss: 0.5118, acc: 85.71%  cls0_acc: 84.73%, cls1_acc: 92.41%\n",
      "Epoch 13/20  loss: 0.4993, acc: 86.53%  cls0_acc: 85.66%, cls1_acc: 92.41%\n",
      "Epoch 14/20  loss: 0.4891, acc: 86.36%  cls0_acc: 85.66%, cls1_acc: 91.14%\n",
      "Epoch 15/20  loss: 0.4791, acc: 86.69%  cls0_acc: 86.22%, cls1_acc: 89.87%\n",
      "Epoch 16/20  loss: 0.4666, acc: 87.34%  cls0_acc: 87.34%, cls1_acc: 87.34%\n",
      "Epoch 17/20  loss: 0.4593, acc: 87.66%  cls0_acc: 87.71%, cls1_acc: 87.34%\n",
      "Epoch 18/20  loss: 0.4519, acc: 87.18%  cls0_acc: 86.78%, cls1_acc: 89.87%\n",
      "Epoch 19/20  loss: 0.4434, acc: 87.34%  cls0_acc: 86.96%, cls1_acc: 89.87%\n",
      "Epoch 20/20  loss: 0.4361, acc: 88.31%  cls0_acc: 88.27%, cls1_acc: 88.61%\n"
     ]
    }
   ],
   "source": [
    "# Training meta model\n",
    "num_epochs = 20\n",
    "num_classes = 2\n",
    "for epoch in range(num_epochs):\n",
    "    meta_model.train()\n",
    "    running_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "    class_correct = [0]*num_classes\n",
    "    class_total   = [0]*num_classes\n",
    "\n",
    "    for Xb, yb in meta_loader:\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = meta_model(Xb)\n",
    "        loss   = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * Xb.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total   += yb.size(0)\n",
    "\n",
    "        # Per-class\n",
    "        for cls in range(num_classes):\n",
    "            mask = (yb == cls)\n",
    "            class_total[cls]   += mask.sum().item()\n",
    "            class_correct[cls] += (preds[mask] == cls).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc  = correct / total * 100\n",
    "    cls0_acc   = class_correct[0] / class_total[0] * 100\n",
    "    cls1_acc   = class_correct[1] / class_total[1] * 100\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}  \"\n",
    "          f\"loss: {epoch_loss:.4f}, acc: {epoch_acc:.2f}%  \"\n",
    "          f\"cls0_acc: {cls0_acc:.2f}%, cls1_acc: {cls1_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b0249a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using test set\n",
    "modelA.eval()\n",
    "modelB.eval()\n",
    "modelC.eval()\n",
    "meta_model.eval()\n",
    "\n",
    "final_preds_test = []\n",
    "with torch.no_grad():\n",
    "  for X, _ in test_loader:\n",
    "    X = X.to(device)\n",
    "\n",
    "    pA = torch.softmax(modelA(X), dim = 1)[:, 1]\n",
    "    pB = torch.softmax(modelB(X), dim = 1)[:, 1]\n",
    "    pC = torch.softmax(modelC(X), dim = 1)[:, 1]\n",
    "\n",
    "    stacked = torch.stack([pA, pB, pC], dim = 1)\n",
    "    logits  = meta_model(stacked.to(device))\n",
    "    preds   = logits.argmax(dim = 1).cpu()\n",
    "    \n",
    "    final_preds_test.append(preds)\n",
    "\n",
    "final_preds_test = torch.cat(final_preds_test)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4d68a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8816855753646677\n",
      "Class 0 accuracy: 88.76%\n",
      "Class 1 accuracy: 84.78%\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9708    0.8876    0.9274       525\n",
      "           1     0.5693    0.8478    0.6812        92\n",
      "\n",
      "    accuracy                         0.8817       617\n",
      "   macro avg     0.7701    0.8677    0.8043       617\n",
      "weighted avg     0.9110    0.8817    0.8907       617\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Getting the ground‐truths from test_loader\n",
    "all_labels_test = []\n",
    "with torch.no_grad():\n",
    "    for _, y in test_loader:\n",
    "        all_labels_test.append(y)\n",
    "final_labels_test = torch.cat(all_labels_test)  # [N_test]\n",
    "\n",
    "# Computing metrics with scikit‐learn\n",
    "y_true = final_labels_test.numpy()\n",
    "y_pred = final_preds_test .numpy()\n",
    "confusion = confusion_matrix(y_true, y_pred)\n",
    "class0_acc = confusion[0, 0] / confusion[0, :].sum()\n",
    "class1_acc = confusion[1, 1] / confusion[1, :].sum()\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "print(f\"Class 0 accuracy: {class0_acc*100:.2f}%\")\n",
    "print(f\"Class 1 accuracy: {class1_acc*100:.2f}%\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred, digits = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d529fd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val label counts: tensor([537,  79])\n",
      "Test label counts: tensor([525,  92])\n"
     ]
    }
   ],
   "source": [
    "# How many samples are in each class for the validation and test sets.\n",
    "print(\"Val label counts:\", torch.bincount(final_labels))\n",
    "print(\"Test label counts:\", torch.bincount(final_labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529e8fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model\n",
    "# PATH = 'Models/meta_model.pth'\n",
    "# torch.save(meta_model.state_dict(), PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
