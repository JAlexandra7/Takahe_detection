{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b788e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Citations\n",
    "# GBIF.org (15 June 2025) GBIF Occurrence Download  https://doi.org/10.15468/dl.mhxdcy\n",
    "# Data was obtained from GBIF-NZ, it includes records from 6 datasets: iNaturalist Research-grade Observations Observation.org, Nature data from around the World, Auckland Museum Land Vertebrates Collection, Xeno-canto - Bird sounds from around the world, NABU|naturgucker, MVZ Egg and Nest Collection (Arctos)\n",
    "# There were 4258 occurances in the original dataset, but this was reduced to 4202 occurances as a result of data cleaning.\n",
    "# Of the original 4258 occurances, 581 were takahe (Porphyrio hochstetteri) and 3621 were pukeko (Porphyrio melanotus subsp. melanotus).\n",
    "# In the cleaned dataset 550 were takahe (Porphyrio hochstetteri) and 3557 were pukeko (Porphyrio melanotus subsp. melanotus).\n",
    "# The data cleaning and downloading of the images was performed in Rstudio.\n",
    "# Made using Python 3.12.9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0803183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading packages, using python 3.12.9\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e535a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_idx.txt\", \"r\") as file:\n",
    "    test_idx = [int(line.strip()) for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35dbef77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[624, 4, 3279, 1936, 2719, 3423, 3913, 2111, 2653, 950, 742, 2352, 1518, 996, 4098, 1794, 1990, 3761, 3422, 1882, 3100, 2899, 2979, 3361, 2061, 3299, 2697, 1805, 1950, 1209, 3921, 979, 1760, 803, 1444, 3434, 2330, 3057, 2987, 2959, 3773, 2301, 49, 3648, 1567, 2230, 2366, 511, 1334, 3883, 81, 1787, 3945, 949, 2446, 2128, 1400, 2264, 3292, 846, 3464, 1227, 2621, 2634, 2701, 3383, 2677, 2151, 934, 3145, 3587, 3264, 2211, 983, 2101, 1362, 3906, 3498, 3160, 2489, 1048, 1878, 1035, 2449, 1839, 3845, 2894, 2685, 985, 3949, 3712, 1328, 3594, 1499, 3679, 280, 3843, 569, 3904, 974, 1407, 1756, 1908, 2617, 2811, 1033, 1144, 2378, 496, 2739, 2131, 3641, 2087, 2830, 32, 967, 1366, 1317, 1657, 3213, 3889, 1164, 3297, 464, 2240, 1203, 2939, 2212, 969, 116, 1924, 4045, 3527, 1972, 992, 3531, 1027, 1078, 2976, 1537, 1174, 2213, 481, 3435, 4070, 3308, 3121, 1091, 542, 3777, 578, 3729, 3104, 1714, 970, 2958, 196, 1833, 3153, 1616, 746, 3310, 2748, 1510, 499, 1699, 1255, 1709, 529, 3672, 2568, 2204, 2584, 910, 3640, 2456, 3900, 203, 3706, 2189, 844, 3026, 957, 2148, 2334, 256, 3579, 3941, 3159, 2605, 3667, 259, 3219, 1345, 694, 3286, 1932, 1798, 3456, 2789, 3878, 855, 150, 2399, 3049, 457, 3732, 221, 638, 1643, 2557, 2767, 956, 3232, 425, 2709, 475, 1307, 3065, 1870, 3946, 2184, 1767, 1773, 3431, 1842, 1598, 575, 18, 691, 146, 3798, 1624, 1648, 1301, 1146, 2153, 2519, 1783, 119, 2083, 3216, 2163, 372, 1346, 764, 3577, 3146, 70, 829, 4078, 3986, 1558, 2479, 966, 1290, 38, 3097, 3289, 756, 1363, 556, 687, 4050, 1623, 1155, 947, 997, 3188, 2972, 3523, 698, 427, 3376, 1574, 4084, 1576, 1956, 207, 952, 1850, 1946, 3975, 3927, 2170, 1662, 3105, 1632, 1473, 3396, 1193, 1349, 3980, 217, 3414, 860, 2581, 2123, 2823, 1595, 384, 2262, 836, 4061, 1861, 1125, 1996, 3631, 2595, 3381, 2995, 1692, 1090, 2112, 2065, 1496, 1620, 3652, 3709, 1262, 2941, 3716, 3661, 3175, 3560, 460, 2505, 1858, 3964, 2515, 3086, 3663, 69, 2279, 1724, 572, 1404, 2478, 3951, 2561, 1846, 3007, 1738, 368, 622, 627, 454, 2304, 348, 3351, 187, 1536, 1208, 1175, 1711, 3647, 2363, 1487, 1011, 3628, 3401, 2542, 3476, 3915, 424, 2450, 717, 1962, 3870, 3793, 549, 1964, 1172, 3403, 2736, 350, 1177, 3693, 2794, 2707, 946, 3540, 3459, 2658, 1411, 2860, 1148, 2659, 2960, 2777, 98, 1131, 330, 693, 1199, 1100, 355, 2673, 737, 1572, 805, 2518, 243, 3748, 2209, 3325, 3837, 1838, 2686, 880, 1770, 309, 3504, 895, 3428, 396, 833, 3753, 2356, 2227, 3274, 1580, 3738, 176, 2907, 2196, 1355, 3388, 1330, 2345, 1266, 3482, 879, 1478, 1941, 2340, 2698, 859, 1212, 1167, 3959, 1772, 429, 4013, 218, 3334, 540, 3856, 1704, 73, 611, 67, 1895, 2130, 757, 3317, 1543, 505, 1866, 3510, 3836, 2818, 2086, 2327, 523, 2880, 2700, 2585, 3786, 3996, 2349, 1206, 2867, 2312, 848, 53, 1467, 3891, 636, 2815, 4074, 3894, 2133, 4093, 3639, 2419, 3368, 254, 2120, 750, 1098, 576, 917, 2294, 920, 2929, 4008, 1719, 2093, 1455, 2054, 3516, 2139, 543, 522, 781, 3865, 2297, 436, 302, 1268, 1684, 2791, 41, 797, 3408, 3387, 1541, 3569, 3822, 1826, 71, 2715, 3149, 190, 678, 3972, 1851, 3490, 2201, 3794, 2259, 2817, 3633, 3316, 2109, 64, 166, 3496, 3339, 1609, 3656, 495, 3407, 153, 1000, 3905, 2699, 1259, 3795, 2343, 799, 1642, 3572, 2594, 3046, 3413, 2321, 2452, 567, 2284, 1530, 2984, 3295, 3380, 715, 189, 3609, 1483, 1264, 173, 888, 626, 3513, 3410, 2769, 3473, 2229, 3447, 2461, 598, 2780, 3005, 3784, 825, 3067, 1977, 2702, 346, 1367, 1168, 3769, 1879, 3832, 1065, 478, 1825, 500, 1533, 650, 201, 3242, 1625, 3111, 3126, 2265, 174, 4096, 3751, 769, 659, 4017, 3179, 3920, 1390, 794, 3311]\n"
     ]
    }
   ],
   "source": [
    "print(test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5fb916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the data by resizing the images, converting the images to tensors and normalizing the images\n",
    "Transform_data = transforms.Compose([\n",
    "    # Resizes images to 600 by 600 pixels\n",
    "    transforms.Resize((600, 600)),\n",
    "    # randomly horizontally flips images\n",
    "    transforms.RandomHorizontalFlip(p = 0.1),\n",
    "    # randomly rotates images\n",
    "    transforms.RandomRotation(degrees = (0, 180)),\n",
    "    # Randomly alters the visual appearance of the images during training\n",
    "    transforms.ColorJitter(brightness = 0.2, contrast = 0.2, saturation = 0.2, hue = 0.1),\n",
    "    # randomly crops a random portion of an image\n",
    "    # transforms.RandomResizedCrop(size = (128, 128)),\n",
    "    # turns images into tensors\n",
    "    transforms.ToTensor(),\n",
    "    # Normalises the data\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# random data augmentation techniques are not used on the validation set to avoid overfitting and \n",
    "V_Transform_data = transforms.Compose([\n",
    "    # Resizes images to 600 by 600 pixels\n",
    "    transforms.Resize((600, 600)),\n",
    "    # turns images into tensors\n",
    "    transforms.ToTensor(),\n",
    "    # Normalises the data\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a747e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing data and splitting the dataset into training, validation and testing sets\n",
    "Images = ImageFolder(root = \"Train_data\", transform = None)\n",
    "\n",
    "# Setting a seed to ensure reproducible results\n",
    "generator = torch.Generator().manual_seed(0)\n",
    "test_ds  = Subset(ImageFolder(\"Train_data\", transform = V_Transform_data),  test_idx)\n",
    "test_loader  = DataLoader(test_ds, batch_size=35, shuffle = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07dc017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the CNN model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Feature extraction\n",
    "        # convolution layer generates a feature map\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 16, kernel_size = 3, stride = 2, padding = 1)\n",
    "        # batch normalisation\n",
    "        self.batch1 = nn.BatchNorm2d(16, momentum = 0.1, affine = True) #  momentum and affine are set to default,\n",
    "        # Using stride = 2 for first conv, then pooling on 2nd conv, then stride = 2 for thrid conv\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 3, stride = 1)\n",
    "        self.batch2 = nn.BatchNorm2d(32, momentum = 0.1, affine = True) #  momentum and affine are set to default\n",
    "        # reduce the size of the feature map\n",
    "        self.pool = nn.MaxPool2d(kernel_size = 5, stride = 5, padding = 0, ceil_mode = True)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3, stride = 2)\n",
    "        self.batch3 = nn.BatchNorm2d(64, momentum = 0.1, affine = True) #  momentum and affine are set to default\n",
    "        \n",
    "        self.pool2 = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # applies a linear transformation to the data\n",
    "        self.fc1 = nn.Linear(in_features = 64, out_features = 32)\n",
    "        self.batchfc1 = nn.BatchNorm1d(32)\n",
    "        self.fc2 = nn.Linear(in_features = 32, out_features = 16)\n",
    "        self.batchfc2 = nn.BatchNorm1d(16)\n",
    "        self.fc3 = nn.Linear(in_features = 16, out_features = 8)\n",
    "        self.batchfc3 = nn.BatchNorm1d(8)\n",
    "        # out_features corresponds to the number of classes\n",
    "        self.fc4 = nn.Linear(in_features = 8, out_features = 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Note that printing is for debugging\n",
    "        #print(x.shape)\n",
    "        # blocks: conv, batch, relu, pooling\n",
    "        x = F.relu(self.pool(self.batch1(self.conv1(x))))\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.batch2(self.conv2(x)))\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.pool(self.batch3(self.conv3(x))))\n",
    "        #print(x.shape)\n",
    "        # Flattens input 'x' by reshaping it into a one dimensional tensor.\n",
    "        x = self.pool2(x)\n",
    "        #print(\"just pooled\")\n",
    "        #print(x.shape)\n",
    "        x = torch.flatten(x, 1)\n",
    "        # linear\n",
    "        #print(x.shape)\n",
    "        #print(\"just flattened\")\n",
    "        x = F.relu(self.batchfc1(self.fc1(x)))\n",
    "        #print(\"lin 1\")\n",
    "        x = F.relu(self.batchfc2(self.fc2(x)))\n",
    "        x = F.relu(self.batchfc3(self.fc3(x)))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "model1 = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd93cca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (batch1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (batch2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool): MaxPool2d(kernel_size=5, stride=5, padding=0, dilation=1, ceil_mode=True)\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (batch3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool2): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (batchfc1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (batchfc2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc3): Linear(in_features=16, out_features=8, bias=True)\n",
       "  (batchfc3): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc4): Linear(in_features=8, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading trained model\n",
    "model = model1\n",
    "model.load_state_dict(torch.load(\"Models/model.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06b24147",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = (\"Pukeko\", \"Takahe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dcfe26b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the Test images: 80 %\n"
     ]
    }
   ],
   "source": [
    "# Checking the overall accuracy of the model on the Test set.\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the Test images: {100 * correct // total} %')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1113e32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class: Pukeko is 86.7 %\n",
      "Accuracy for class: Takahe is 43.5 %\n"
     ]
    }
   ],
   "source": [
    "# Checking the accuracy for each class on the Test set.\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c354a9de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e84018",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
